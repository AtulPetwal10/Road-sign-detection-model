{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwVMHe6z2Sko"
      },
      "outputs": [],
      "source": [
        "pip install ultralytics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1V1ihba9Ps-e"
      },
      "outputs": [],
      "source": [
        "pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZ3Vgi2G2MDz"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "from google.colab import drive\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7CxHYKw41Tv"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# # Path to the dataset folder\n",
        "# path = '/content/drive/My Drive/Traffic and Road Signs.v1i.yolov5pytorch'\n",
        "\n",
        "# # List of the files in the folder\n",
        "# files = os.listdir(path)\n",
        "# print(\"Files in the folder:\", files)\n",
        "\n",
        "# # 'data.yaml' check\n",
        "# if 'data.yaml' in files:\n",
        "#     print(\"data.yaml found!\")\n",
        "# else:\n",
        "#     print(\"data.yaml is missing from the folder. Please ensure it is in the correct location.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JbAdCFocr6Tg"
      },
      "outputs": [],
      "source": [
        "# from ultralytics import YOLO\n",
        "\n",
        "# #pre trained yolo model\n",
        "# model = YOLO('yolov8n.pt')\n",
        "\n",
        "# #training the model\n",
        "# data_yaml_path = '/content/drive/My Drive/Traffic and Road Signs.v1i.yolov5pytorch/data.yaml'\n",
        "\n",
        "\n",
        "# model.train(data=data_yaml_path, epochs=1, imgsz=640)\n",
        "\n",
        "# # Evaluate the model\n",
        "# results = model.val()\n",
        "\n",
        "\n",
        "# model.export(format='onnx')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70HNpw477APG"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import gradio as gr\n",
        "\n",
        "# Loading model best1 and best2\n",
        "model1 = YOLO('best2.pt')\n",
        "model2 = YOLO('best1.pt')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0sRz3oLUAvyN"
      },
      "outputs": [],
      "source": [
        "\n",
        "#this function will process the image and predict whether there exist road sign in image or not\n",
        "def process_image(image):\n",
        "\n",
        "\n",
        "    results1 = model1(image)\n",
        "    annotated_image1 = results1[0].plot()\n",
        "\n",
        "    # Convert BGR to RGB for proper colors as RGB is more compatible than BRG\n",
        "    annotated_image1 = cv2.cvtColor(annotated_image1, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "\n",
        "    results2 = model2(image)\n",
        "    annotated_image2 = results2[0].plot()\n",
        "\n",
        "\n",
        "    annotated_image2 = cv2.cvtColor(annotated_image2, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    return annotated_image1, annotated_image2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jgR_wQaWA3wG"
      },
      "outputs": [],
      "source": [
        "def process_video(video_path):\n",
        "\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        return \"Error: Couldn't open video.\"\n",
        "\n",
        "    # creating a folder where videos output can be saved if folder dosent exsit os.makedirs() will make the folder for us\n",
        "    temp_dir = \"./temp\"\n",
        "    if not os.path.exists(temp_dir):\n",
        "        os.makedirs(temp_dir)\n",
        "    #outout_model1.mp4 and output_model2.mp4 is name of of output videos\n",
        "    output_video_path1 = os.path.join(temp_dir, \"output_model1.mp4\")\n",
        "    output_video_path2 = os.path.join(temp_dir, \"output_model2.mp4\")\n",
        "\n",
        "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "\n",
        "    out1 = cv2.VideoWriter(output_video_path1, fourcc, fps, (frame_width, frame_height))\n",
        "    out2 = cv2.VideoWriter(output_video_path2, fourcc, fps, (frame_width, frame_height))\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "\n",
        "        results1 = model1.predict(frame)\n",
        "        annotated_frame1 = results1[0].plot()\n",
        "        out1.write(annotated_frame1)\n",
        "\n",
        "        # Inference with the second model\n",
        "        results2 = model2.predict(frame)\n",
        "        annotated_frame2 = results2[0].plot()\n",
        "        out2.write(annotated_frame2)\n",
        "\n",
        "    cap.release()\n",
        "    out1.release()\n",
        "    out2.release()\n",
        "\n",
        "    return output_video_path1, output_video_path2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKEJEfry9jI6"
      },
      "outputs": [],
      "source": [
        "# Main function to handle image and video input\n",
        "def process_input(input_path, output_folder):\n",
        "\n",
        "    if os.path.isfile(input_path):\n",
        "        file_ext = os.path.splitext(input_path)[-1].lower()\n",
        "        if file_ext in ['.jpg', '.jpeg', '.png', '.bmp']:\n",
        "            process_image(input_path, output_folder)\n",
        "        elif file_ext in ['.mp4', '.avi', '.mov', '.mkv']:\n",
        "            process_video(input_path, output_folder)\n",
        "        else:\n",
        "            print(\"Unsupported file format!\")\n",
        "    else:\n",
        "        print(\"Error: Input file does not exist.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6sUyVgzBA-r6"
      },
      "outputs": [],
      "source": [
        "\n",
        "#code for front end using gradio\n",
        "def gradio_interface(input_file):\n",
        "    file_ext = os.path.splitext(input_file.name)[-1].lower()\n",
        "    if file_ext in ['.jpg', '.jpeg', '.png', '.bmp']:\n",
        "        # Process image\n",
        "        image = cv2.imread(input_file.name)\n",
        "        annotated_image1, annotated_image2 = process_image(image)\n",
        "        return annotated_image1, annotated_image2\n",
        "    elif file_ext in ['.mp4', '.avi', '.mov', '.mkv']:\n",
        "        # Process video\n",
        "        video_path1, video_path2 = process_video(input_file.name)\n",
        "        return video_path1, video_path2\n",
        "    else:\n",
        "        return \"Unsupported file format!\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "ktA2H3kH5eG9",
        "outputId": "b4ce8e09-22cc-4ef7-a46c-0e15236e1a5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://486300be7bd224a556.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://486300be7bd224a556.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# defining the gradio application\n",
        "interface = gr.Interface(\n",
        "    fn=gradio_interface,\n",
        "    inputs=gr.File(label=\"Upload Image or Video\"),\n",
        "    outputs=[\n",
        "        gr.Image(label=\"Model 1 Annotated Image\"),\n",
        "        gr.Image(label=\"Model 2 Annotated Image\"),\n",
        "    ],\n",
        "    title=\"YOLOv8 Dual Model Prediction\",\n",
        "    description=\"Upload an image or video to see predictions from two YOLOv8 models.\"\n",
        ")\n",
        "\n",
        "# launching the application\n",
        "interface.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxglANKaTwUT"
      },
      "outputs": [],
      "source": [
        "#this code is for live time predection for image and videos predection code is present above\n",
        "\n",
        "import cv2\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# Step 1: Load YOLO models\n",
        "model_path1 = 'best2.pt'  \n",
        "model_path2 = 'best1.pt'  \n",
        "model1 = YOLO(model_path1) \n",
        "model2 = YOLO(model_path2)  \n",
        "\n",
        "\n",
        "cap = cv2.VideoCapture(0) \n",
        "\n",
        "if not cap.isOpened():\n",
        "    print(\"Error: Could not access the webcam.\")\n",
        "    exit()\n",
        "\n",
        "while True:\n",
        "\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        print(\"Failed to grab frame\")\n",
        "        break\n",
        "    \n",
        "    \n",
        "    results1 = model1.predict(frame)  \n",
        "    annotated_frame1 = results1[0].plot() \n",
        "\n",
        "    \n",
        "    results2 = model2.predict(frame)  \n",
        "    annotated_frame2 = results2[0].plot()  \n",
        "\n",
        "    \n",
        "    combined_frame = cv2.hconcat([annotated_frame1, annotated_frame2])\n",
        "\n",
        "\n",
        "    cv2.imshow('Real-Time YOLO Prediction', combined_frame)\n",
        "\n",
        "    \n",
        "    if cv2.waitKey(1) & 0xFF == 27:\n",
        "        break\n",
        "\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
